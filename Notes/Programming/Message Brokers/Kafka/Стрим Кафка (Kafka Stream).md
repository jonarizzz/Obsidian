Kafka Streams – это [[{TODO} Библиотека (Library)||библиотека]] для обработки потоков данных в [[Кафка (Apache Kafka)||Apache Kafka]]. Она позволяет разрабатывать приложения, которые читают, обрабатывают и записывают данные в [[Кафка (Apache Kafka)||Kafka]] в режиме реального времени.


### Основные особенности

- **Легковесность** – это просто [[{TODO} Java||Java]]-[[{TODO} Библиотека (Library)||библиотека]], не требующая отдельного сервера.
- **[[Масштабирование (Скейлинг, Scaling)||Горизонтальное масштабирование]]** – можно запустить несколько инстансов для параллельной обработки.
- **Сохранение состояния** – поддерживает RocksDB для хранения промежуточных результатов.
- **[[Транзакция (Transaction)||Транзакции]] и Exactly-Once Processing (EOS)** – поддержка гарантированной доставки сообщений.
- **Интеграция с [[Кафка (Apache Kafka)||Kafka]]** – поток обрабатывается прямо из [[Топик Кафка (Kafka Topic)||топиков Kafka]].


### Как работает

- Читает данные из [[Кафка (Apache Kafka)||Kafka]] ([[Потребитель Кафка (Kafka Consumer)||Consumer API]])
- Преобразует, фильтрует, агрегирует и объединяет потоки
- Записывает обработанные данные обратно в [[Кафка (Apache Kafka)||Kafka]] ([[Производитель Кафка (Kafka Producer)||Producer API]])

[[Стрим Кафка (Kafka Stream)||Kafka Streams]] = [[Потребитель Кафка (Kafka Consumer)||Consumer]] + Business Logic + [[Производитель Кафка (Kafka Producer)||Producer]]


### Основные концепции

- **KStream** – непрерывный поток данных (каждое [[Сообщение Кафка (Kafka Message)||сообщение]] обрабатывается отдельно).
- **KTable** – представление данных как “таблица” (аналог [[Таблица БД (Database Table, DB Table)||SQL-таблицы]], обновляется при изменениях).
- **GlobalKTable** – глобальная таблица, доступная во всех инстансах приложения.
- **State Store** – хранилище для локального [[Кэш (Cache)||кеширования]] данных.


### Пример кода (фильтрация сообщений)

```java
StreamsBuilder builder = new StreamsBuilder();

KStream<String, String> stream = builder.stream("input-topic");

stream.filter((key, value) -> value.contains("important")).to("filtered-topic");

KafkaStreams streams = new KafkaStreams(builder.build(), properties);

streams.start();
```

Этот код:

- Читает сообщения из input-topic.
- Фильтрует сообщения, содержащие “important”.
- Записывает их в filtered-topic.


### Когда использовать

- **Реалтайм-анализ данных** ([[Мониторинг (Monitoring)||мониторинг]], алерты)
- **Обогащение данных** (добавление информации из других источников)
- **Агрегация и группировка** (метрики, статистика)
- **[[{TODO} ETL (Extract, Transform, Load)||ETL-пайплайны]]** (перемещение и трансформация данных)

  

Если нужно сложное распределенное управление потоками, лучше использовать [[Apache Flink||Apache Flink]].